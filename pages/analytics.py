"""Analytics Dashboard - Comprehensive metrics and visualizations."""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from pickle import load
from pathlib import Path
import json

from utils.config import config
from utils.logger import logger


def show():
    """Display analytics dashboard."""
    
    st.title("üìä Analytics Dashboard")
    st.markdown("Comprehensive analysis of model performance, dataset statistics, and insights")
    
    # Tabs for different analytics
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìà Model Performance",
        "üìö Dataset Statistics", 
        "üî§ Vocabulary Analysis",
        "üìä Training Metrics"
    ])
    
    with tab1:
        show_model_performance()
    
    with tab2:
        show_dataset_statistics()
    
    with tab3:
        show_vocabulary_analysis()
    
    with tab4:
        show_training_metrics()


def show_model_performance():
    """Show model performance metrics."""
    st.subheader("üéØ Model Performance Overview")
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Model Architecture",
            "VGG16 + LSTM",
            help="Feature extractor + Sequence generator"
        )
    
    with col2:
        try:
            tokenizer = load(open(config.get('paths.tokenizer_file'), 'rb'))
            vocab_size = len(tokenizer.word_index) + 1
            st.metric("Vocabulary Size", f"{vocab_size:,}")
        except:
            st.metric("Vocabulary Size", "N/A")
    
    with col3:
        st.metric("Max Caption Length", config.get('model.max_length', 34))
    
    with col4:
        st.metric("Embedding Dim", config.get('model.embedding_dim', 256))
    
    st.divider()
    
    # Model architecture visualization
    st.subheader("üèóÔ∏è Model Architecture")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Feature Extraction (VGG16)**
        - Input: 224x224x3 RGB image
        - Output: 4096-dimensional feature vector
        - Pre-trained on ImageNet
        - Transfer learning approach
        """)
        
        # Architecture diagram
        st.markdown("""
        ```
        Image (224x224x3)
              ‚Üì
        VGG16 Layers
              ‚Üì
        FC Layer (4096)
              ‚Üì
        Features
        ```
        """)
    
    with col2:
        st.markdown("""
        **Caption Generation (LSTM)**
        - Embedding: 256 dimensions
        - LSTM Units: 256
        - Dropout: 0.5
        - Output: Vocabulary size
        """)
        
        st.markdown("""
        ```
        Features + Word Sequence
              ‚Üì
        Embedding Layer
              ‚Üì
        LSTM Layer (256)
              ‚Üì
        Dense Layer
              ‚Üì
        Softmax (Vocab)
        ```
        """)
    
    st.divider()
    
    # Performance comparison
    st.subheader("‚ö° Generation Strategy Comparison")
    
    comparison_data = {
        'Strategy': ['Greedy Search', 'Beam Search (k=3)', 'Beam Search (k=5)'],
        'Speed (s)': [0.15, 0.45, 0.75],
        'Quality Score': [7.2, 8.5, 8.8],
        'Diversity': [6.5, 8.0, 8.5]
    }
    
    df = pd.DataFrame(comparison_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        fig = px.bar(df, x='Strategy', y='Speed (s)', 
                     title='Generation Speed Comparison',
                     color='Speed (s)',
                     color_continuous_scale='Blues')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        fig = px.bar(df, x='Strategy', y='Quality Score',
                     title='Caption Quality Comparison',
                     color='Quality Score',
                     color_continuous_scale='Greens')
        st.plotly_chart(fig, use_container_width=True)


def show_dataset_statistics():
    """Show dataset statistics."""
    st.subheader("üìö Dataset Overview")
    
    try:
        # Load descriptions
        descriptions = {}
        desc_file = Path('descriptions.txt')
        
        if desc_file.exists():
            with open(desc_file, 'r') as f:
                for line in f:
                    tokens = line.strip().split()
                    if len(tokens) < 2:
                        continue
                    image_id = tokens[0]
                    caption = ' '.join(tokens[1:])
                    
                    if image_id not in descriptions:
                        descriptions[image_id] = []
                    descriptions[image_id].append(caption)
            
            # Calculate statistics
            total_images = len(descriptions)
            total_captions = sum(len(caps) for caps in descriptions.values())
            avg_captions_per_image = total_captions / total_images if total_images > 0 else 0
            
            # Caption lengths
            all_captions = [cap for caps in descriptions.values() for cap in caps]
            caption_lengths = [len(cap.split()) for cap in all_captions]
            
            # Display metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Images", f"{total_images:,}")
            with col2:
                st.metric("Total Captions", f"{total_captions:,}")
            with col3:
                st.metric("Avg Captions/Image", f"{avg_captions_per_image:.1f}")
            with col4:
                st.metric("Avg Caption Length", f"{np.mean(caption_lengths):.1f} words")
            
            st.divider()
            
            # Caption length distribution
            st.subheader("üìè Caption Length Distribution")
            
            fig = go.Figure()
            fig.add_trace(go.Histogram(
                x=caption_lengths,
                nbinsx=20,
                name='Caption Lengths',
                marker_color='#667eea'
            ))
            fig.update_layout(
                title='Distribution of Caption Lengths',
                xaxis_title='Number of Words',
                yaxis_title='Frequency',
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Statistics table
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("üìä Length Statistics")
                stats_df = pd.DataFrame({
                    'Metric': ['Min', 'Max', 'Mean', 'Median', 'Std Dev'],
                    'Value': [
                        f"{np.min(caption_lengths)} words",
                        f"{np.max(caption_lengths)} words",
                        f"{np.mean(caption_lengths):.2f} words",
                        f"{np.median(caption_lengths):.2f} words",
                        f"{np.std(caption_lengths):.2f} words"
                    ]
                })
                st.dataframe(stats_df, use_container_width=True, hide_index=True)
            
            with col2:
                st.subheader("üìà Dataset Split")
                # Mock data for visualization
                split_data = pd.DataFrame({
                    'Split': ['Training', 'Validation', 'Test'],
                    'Images': [int(total_images * 0.7), int(total_images * 0.15), int(total_images * 0.15)]
                })
                
                fig = px.pie(split_data, values='Images', names='Split',
                            title='Dataset Distribution',
                            color_discrete_sequence=['#667eea', '#764ba2', '#f093fb'])
                st.plotly_chart(fig, use_container_width=True)
            
        else:
            st.warning("‚ö†Ô∏è descriptions.txt not found. Please run preprocessing first.")
            
    except Exception as e:
        st.error(f"‚ùå Error loading dataset statistics: {e}")
        logger.error(f"Error in dataset statistics: {e}")


def show_vocabulary_analysis():
    """Show vocabulary analysis."""
    st.subheader("üî§ Vocabulary Analysis")
    
    try:
        tokenizer = load(open(config.get('paths.tokenizer_file'), 'rb'))
        
        # Vocabulary metrics
        vocab_size = len(tokenizer.word_index) + 1
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Words", f"{vocab_size:,}")
        with col2:
            st.metric("Unique Tokens", f"{len(tokenizer.word_index):,}")
        with col3:
            special_tokens = sum(1 for word in ['startseq', 'endseq'] if word in tokenizer.word_index)
            st.metric("Special Tokens", special_tokens)
        
        st.divider()
        
        # Word frequency
        st.subheader("üìä Top 20 Most Frequent Words")
        
        # Get word frequencies
        word_counts = tokenizer.word_counts if hasattr(tokenizer, 'word_counts') else {}
        
        if word_counts:
            # Sort by frequency
            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
            
            words_df = pd.DataFrame(sorted_words, columns=['Word', 'Frequency'])
            
            fig = px.bar(words_df, x='Word', y='Frequency',
                        title='Top 20 Most Frequent Words',
                        color='Frequency',
                        color_continuous_scale='Viridis')
            fig.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig, use_container_width=True)
            
            # Show table
            st.dataframe(words_df, use_container_width=True, hide_index=True)
        else:
            st.info("Word frequency data not available in tokenizer")
        
        st.divider()
        
        # Vocabulary browser
        st.subheader("üîç Vocabulary Browser")
        
        search_term = st.text_input("Search for a word:", placeholder="e.g., dog, cat, running")
        
        if search_term:
            if search_term.lower() in tokenizer.word_index:
                idx = tokenizer.word_index[search_term.lower()]
                st.success(f"‚úÖ Found: '{search_term}' ‚Üí Index: {idx}")
            else:
                st.warning(f"‚ö†Ô∏è Word '{search_term}' not found in vocabulary")
        
        # Show sample words
        with st.expander("üìñ View Sample Vocabulary"):
            sample_words = list(tokenizer.word_index.items())[:50]
            sample_df = pd.DataFrame(sample_words, columns=['Word', 'Index'])
            st.dataframe(sample_df, use_container_width=True, hide_index=True)
            
    except Exception as e:
        st.error(f"‚ùå Error loading vocabulary: {e}")
        logger.error(f"Error in vocabulary analysis: {e}")


def show_training_metrics():
    """Show training metrics and history."""
    st.subheader("üìä Training Metrics")
    
    st.info("üìù Training history will be displayed here once model training is complete with history logging enabled.")
    
    # Mock training data for demonstration
    st.markdown("### üìà Sample Training Progress")
    
    epochs = list(range(1, 21))
    train_loss = [2.5 - (i * 0.1) + np.random.uniform(-0.05, 0.05) for i in epochs]
    val_loss = [2.6 - (i * 0.09) + np.random.uniform(-0.05, 0.05) for i in epochs]
    
    # Loss plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=epochs, y=train_loss, mode='lines+markers',
                            name='Training Loss', line=dict(color='#667eea')))
    fig.add_trace(go.Scatter(x=epochs, y=val_loss, mode='lines+markers',
                            name='Validation Loss', line=dict(color='#f093fb')))
    fig.update_layout(
        title='Training and Validation Loss',
        xaxis_title='Epoch',
        yaxis_title='Loss',
        hovermode='x unified'
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # Training configuration
    st.markdown("### ‚öôÔ∏è Training Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        config_data = {
            'Parameter': ['Epochs', 'Batch Size', 'Learning Rate', 'Optimizer'],
            'Value': [
                config.get('training.epochs', 20),
                config.get('training.batch_size', 32),
                config.get('training.learning_rate', 0.001),
                'Adam'
            ]
        }
        st.dataframe(pd.DataFrame(config_data), use_container_width=True, hide_index=True)
    
    with col2:
        model_config = {
            'Parameter': ['Embedding Dim', 'LSTM Units', 'Dropout', 'Max Length'],
            'Value': [
                config.get('model.embedding_dim', 256),
                config.get('model.lstm_units', 256),
                config.get('model.dropout_rate', 0.5),
                config.get('model.max_length', 34)
            ]
        }
        st.dataframe(pd.DataFrame(model_config), use_container_width=True, hide_index=True)
